@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={arXiv preprint arXiv:2302.04761},
  year={2023}
}

@article{dutton2023can,
  title={Can foundation models perform zero-shot task specification?},
  author={Dutton, Sam and others},
  journal={arXiv preprint},
  year={2023}
}

@article{light2023toward,
  title={Toward human-level performance on all of atari games},
  author={Light, A and others},
  journal={arXiv preprint},
  year={2023}
}

@article{zhong2023memory,
  title={Memorybank: Enhancing large language models with long-term memory},
  author={Zhong, Wanjun and others},
  journal={arXiv preprint arXiv:2305.10250},
  year={2023}
}

